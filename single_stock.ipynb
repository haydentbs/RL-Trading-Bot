{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from environments.single_stock import SingleStockEnv\n",
    "from model_architecture.DQN_all import DQNAgents\n",
    "from agent_training.train import train_agent\n",
    "from visualisation.visualisation import visualize_trades_with_benchmark\n",
    "from data_preperation.data_perp import prepare_data\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                                Open       High        Low      Close  \\\n",
      "Date                                                                    \n",
      "2020-02-27 00:00:00-05:00  35.583489  36.290331  34.488744  34.514606   \n",
      "2020-02-28 00:00:00-05:00  33.359521  34.954226  33.057818  34.419785   \n",
      "2020-03-02 00:00:00-05:00  34.609430  35.505911  33.549163  35.488670   \n",
      "2020-03-03 00:00:00-05:00  35.566248  36.117933  34.230144  34.514606   \n",
      "2020-03-04 00:00:00-05:00  35.100766  35.730028  34.307722  35.678307   \n",
      "\n",
      "                             Volume  Dividends  Stock Splits        RSI  \\\n",
      "Date                                                                      \n",
      "2020-02-27 00:00:00-05:00  51442900        0.0           0.0  18.407578   \n",
      "2020-02-28 00:00:00-05:00  80188000        0.0           0.0  19.266035   \n",
      "2020-03-02 00:00:00-05:00  48864300        0.0           0.0  21.311419   \n",
      "2020-03-03 00:00:00-05:00  49294700        0.0           0.0  18.194535   \n",
      "2020-03-04 00:00:00-05:00  30022100        0.0           0.0  21.226317   \n",
      "\n",
      "                                MA20       MA50  MA_ratio  Volatility  \n",
      "Date                                                                   \n",
      "2020-02-27 00:00:00-05:00  40.068495  40.694390  0.984620    0.023622  \n",
      "2020-02-28 00:00:00-05:00  39.753432  40.595873  0.979248    0.023492  \n",
      "2020-03-02 00:00:00-05:00  39.546552  40.510860  0.976196    0.024600  \n",
      "2020-03-03 00:00:00-05:00  39.266831  40.402944  0.971880    0.024721  \n",
      "2020-03-04 00:00:00-05:00  38.998318  40.297080  0.967770    0.025487  \n"
     ]
    }
   ],
   "source": [
    "df = prepare_data('CSCO')\n",
    "\n",
    "env = SingleStockEnv(df)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "New best reward! Portfolio Value: 152162.5146810295 Total Reward: 306.2807478818631\n",
      "Episode: 10, Reward: 320.92, Portfolio Value: $145428.10, Epsilon: 0.01\n",
      "New best reward! Portfolio Value: 145428.10429516927 Total Reward: 320.92265748823615\n",
      "New best reward! Portfolio Value: 158382.47610544672 Total Reward: 355.6241852237578\n",
      "Episode: 20, Reward: 283.86, Portfolio Value: $153207.07, Epsilon: 0.01\n",
      "New best reward! Portfolio Value: 160814.37853646785 Total Reward: 361.6627328095262\n",
      "New best reward! Portfolio Value: 164583.05825444034 Total Reward: 383.59350442986323\n",
      "New best reward! Portfolio Value: 169389.4945191704 Total Reward: 390.4075568958165\n",
      "Episode: 30, Reward: 322.28, Portfolio Value: $153595.92, Epsilon: 0.01\n",
      "Episode: 40, Reward: 344.91, Portfolio Value: $158031.22, Epsilon: 0.01\n",
      "New best reward! Portfolio Value: 168372.16368392343 Total Reward: 396.58699021325197\n",
      "Episode: 50, Reward: 211.33, Portfolio Value: $143581.18, Epsilon: 0.01\n",
      "New best reward! Portfolio Value: 171759.7902266783 Total Reward: 458.8449290148596\n",
      "Episode: 60, Reward: 291.49, Portfolio Value: $148527.43, Epsilon: 0.01\n"
     ]
    }
   ],
   "source": [
    "# Create agent\n",
    "state_size = env.observation_space.shape[0]\n",
    "action_size = env.action_space.n\n",
    "agent = DQNAgents(state_size, action_size, model_type='standard')\n",
    "\n",
    "rewards_history, portfolio_history, best_state_dict = train_agent(env, agent, episodes=500)\n",
    "\n",
    "agent.policy_net.load_state_dict(best_state_dict)\n",
    "\n",
    "visualize_trades_with_benchmark(env, agent, df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "agent.policy_net.load_state_dict(best_state_dict)\n",
    "\n",
    "visualize_trades_with_benchmark(env, agent, df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to compare different models\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "\n",
    "def compare_models(env, episodes=100):\n",
    "    state_size = env.observation_space.shape[0]\n",
    "    action_size = env.action_space.n\n",
    "    \n",
    "    models = {\n",
    "        'Standard DQN': DQNAgents(state_size, action_size, model_type='standard'),\n",
    "        'Dueling DQN': DQNAgents(state_size, action_size, model_type='dueling'),\n",
    "        'LSTM DQN': DQNAgents(state_size, action_size, model_type='lstm')\n",
    "    }\n",
    "    \n",
    "    results = {}\n",
    "    \n",
    "    for model_name, agent in models.items():\n",
    "        print(f\"\\nTraining {model_name}...\")\n",
    "        rewards_history, portfolio_history, _ = train_agent(env, agent, episodes=episodes)\n",
    "        results[model_name] = {\n",
    "            'rewards': rewards_history,\n",
    "            'portfolio': portfolio_history\n",
    "        }\n",
    "    \n",
    "    # Plot comparison results\n",
    "    plt.figure(figsize=(15, 10))\n",
    "    \n",
    "    # Plot rewards\n",
    "    plt.subplot(2, 1, 1)\n",
    "    for model_name, data in results.items():\n",
    "        plt.plot(data['rewards'], label=model_name)\n",
    "    plt.title('Training Rewards Comparison')\n",
    "    plt.xlabel('Episode')\n",
    "    plt.ylabel('Total Reward')\n",
    "    plt.legend()\n",
    "    \n",
    "    # Plot portfolio values\n",
    "    plt.subplot(2, 1, 2)\n",
    "    for model_name, data in results.items():\n",
    "        plt.plot(data['portfolio'], label=model_name)\n",
    "    plt.title('Portfolio Value Comparison')\n",
    "    plt.xlabel('Episode')\n",
    "    plt.ylabel('Portfolio Value ($)')\n",
    "    plt.legend()\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "    \n",
    "    # Print final statistics\n",
    "    print(\"\\nFinal Statistics:\")\n",
    "    for model_name, data in results.items():\n",
    "        final_reward = data['rewards'][-1]\n",
    "        final_portfolio = data['portfolio'][-1]\n",
    "        print(f\"\\n{model_name}:\")\n",
    "        print(f\"Final Reward: {final_reward:.2f}\")\n",
    "        print(f\"Final Portfolio Value: ${final_portfolio:.2f}\")\n",
    "        print(f\"Average Reward: {np.mean(data['rewards']):.2f}\")\n",
    "        print(f\"Average Portfolio Value: ${np.mean(data['portfolio']):.2f}\")\n",
    "    \n",
    "    return results\n",
    "\n",
    "df = prepare_data('CSCO')  # or any other stock\n",
    "env = SingleStockEnv(df)\n",
    "results = compare_models(env, episodes=300)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "rl",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
